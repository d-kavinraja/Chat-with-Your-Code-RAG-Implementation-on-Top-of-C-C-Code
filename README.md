# Chat-with-Your-Code-RAG-Implementation-on-Top-of-C-C-Code

This project implements a Retrieval-Augmented Generation (RAG) system that allows users to interact with the LPrint C/C++ codebase in natural language. By combining chunking, embeddings, and a conversational interface, it enables developers to query the code, understand its structure, and get explanations directly from an AI assistant.

ðŸš€ Features

Code Chunking â€“ Splits large source files into manageable text segments.

Embeddings â€“ Generates vector representations of code chunks for semantic search.

RAG Pipeline â€“ Retrieves relevant chunks based on user queries and augments the LLM response.

Interactive Chat â€“ Provides a chatbot-style interface to ask questions about the repository.

Preprocessed Data â€“ Includes stored embeddings (.npy) and JSON mappings for quick setup.

ðŸ“‚ Project Structure

app.py â†’ Main application entry point

chat_with_lprint.py â†’ Chat interface with RAG integration

chunk_lprint_code.py â†’ Splits code into chunks

embed_chunks.py â†’ Generates embeddings for code chunks

lprint_chunks.json / lprint_texts.json / lprint_embeddings.npy â†’ Preprocessed dataset files

requirements.txt â†’ Dependencies list

README.md â†’ Documentation

LICENSE â†’ License information

ðŸ”§ Tech Stack

Python 3

LangChain (for RAG pipeline)

ChromaDB (vector database for embeddings)

Ollama / TinyLlama (LLM for code Q&A)

ðŸ“Œ Use Case

This project is useful for:

Developers trying to understand large C/C++ repositories.

Students learning how to integrate RAG with codebases.

Building interactive assistants for open-source projects.
